{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2544c477-ca8f-4fa1-ae86-f96023ee0a19",
      "metadata": {},
      "outputs": [],
      "source": [
        "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n</center>\n\n# **Heart Failure Prediction**\n\n# Lab 5. Model Evaluation and Refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32abd3f5-6271-470a-9fa2-1e6274dd07a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d33b43-5f26-4307-89d0-db33cc00464b",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab, you will explore the process of constructing training and test data sets, and learn how to utilize various classifiers for fitting purposes. The focus will be on evaluating the accuracy of these classifiers and conducting error analysis. Additionally, we will delve into the concept of ensemble learning and discover how to combine classifiers through Pipeline to create a comprehensive model. Through a practical example involving patient classification in the medical field, we will compare the accuracy of different classifiers and their ensembles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf50885-bdc1-4ff1-bca0-2eaf78d205f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "Estimated time needed: **30** minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "097d7a3b-1a6f-4319-a427-44daa1671533",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Objectives\n\nAfter completing this lab you will be able to:\n\n* Conduct basic data analysis\n* Calculate new and change column types\n* Divide the DataSet into training and test\n* Use cross-validation\n* Solve the over-sampling problem \n* Use different machine learning classification methods\n* Combine classifiers into ensemble\n* Calculate accuracy and analyze errors\n* Combine all stages of data analysis with Pipeline\n* Demonstrate how classifiers and ensembles can be utilized for medical purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b81cd8b-2fd3-40b1-bd9a-570ce7f22983",
      "metadata": {},
      "outputs": [],
      "source": [
        "<h2>Table of Contents</h2>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"#prep\">Data pre-preparation</a></li>\n        <li><a href=\"#pipeline\">Pipiline Classification</a>\n             <ul>\n                 <li><a href=\"#logistic\">Logistic Regression</a></li>\n                 <li><a href=\"#cross_val\">Cross-validation</a></li>\n                 <li><a href=\"#accuracy\">Accuracy</a></li>\n            </ul>\n        </li>\n        <li><a href=\"#over_samp\">Over-sampling problem</a></li>\n        <li><a href=\"#ensemble\">Ensemble of classifiers</a></li>\n    </ol>\n</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f02f7bc3-2ef8-45ac-9fec-6ee7579870e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Materials and methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06dff1e3-185b-4296-8be7-d278ced25130",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab, we will learn how to download and pre-prepare data, classify and combine classifiers into an ensemble.\nThis lab consists of the following steps:\n* Download data - download and display data from a file\n* Preliminary data preparation - preliminary analysis of data structure, change of data structure and tables\n* Pipeline classification - classification and analysis by grouping stages\n    * Logistic regression - classification and analysis of accuracy and errors using logistic regression\n    * Over-sampling problem - solve the problem of uneven distribution of data\n    * Ensemble of classifiers - study various classifiers and methods of combining them into an ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0000ea64-331c-455c-8905-b1cfc22dfe3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "The statistical data was obtained from the https://www.kaggle.com/datasets/asgharalikhan/mortality-rate-heart-patient-pakistan-hospital. This DataSet released under CC0: Public Domain license that allow of copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59b295d-7114-4772-991c-d6dd0483a711",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Prerequisites\n* [Python](https://www.python.org) - middle level\n* [Pandas](https://pandas.pydata.org) - middle level \n* [Matplotlib](https://matplotlib.org) - basic level\n* [SeaBorn](https://seaborn.pydata.org) - basic level\n* [Scikit-Learn](https://scikit-learn.org/stable/) - middle level "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761d27ed-f9c9-464d-9497-cc6aaf61333a",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import Libraries/Define Auxiliary Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a5f07b-e6ad-4c7e-a5cc-100e7b6da95d",
      "metadata": {},
      "outputs": [],
      "source": [
        "Libraries such as Pandas, MatplotLib, SeaBorn, Scikit-Learn, imbalanced-learn should be installed."
      ]
    },
    {
      "cell_type": "code",
      "id": "08969ea5-47cf-4715-81f1-6dd1efa0abff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge pandas"
      ]
    },
    {
      "cell_type": "code",
      "id": "480a4fc5-ec21-4fc1-a9ce-360f32023246",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "id": "12ffbfd3-5daa-4980-b49f-f37b40ffc875",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c conda-forge seaborn "
      ]
    },
    {
      "cell_type": "code",
      "id": "2a2e4bef-d630-4480-9a72-65de7e030b35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install -c intel scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "id": "cbd254bf-01a7-4a40-9f4a-57d461575b6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "conda install -c conda-forge imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f188c63-020d-4640-a439-c5debc059ef8",
      "metadata": {},
      "outputs": [],
      "source": [
        "Some libraries should be imported before you can begin."
      ]
    },
    {
      "cell_type": "code",
      "id": "a00edeb2-dca8-4909-a4f2-e27049b7043b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler                         \nfrom sklearn.compose import make_column_transformer\nfrom sklearn import set_config\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.metrics import plot_confusion_matrix\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ab01a8-0b2e-4d52-9be7-e4dc18a1c528",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's disable warnings by **[warnings.filterwarnings()](https://docs.python.org/3/library/warnings.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "191db4f7-b6e9-41fa-ad6c-0f0a48954f01",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb94819-f42a-414f-ab35-83814e97ac87",
      "metadata": {},
      "outputs": [],
      "source": [
        "Further specify the value of the precision parameter equal to 2 to display two decimal signs (instead of 6 as default) by and  **[pd.options.display](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)**."
      ]
    },
    {
      "cell_type": "code",
      "id": "e13885d8-5408-4898-81c1-ce77a5031bd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78523543-e401-4593-9bb9-0e2bf6dfdb7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Data pre-preparation <a id=\"prep\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af859bdb-0768-4169-87b6-8b0be6f1dc74",
      "metadata": {},
      "outputs": [],
      "source": [
        "The next step is to download the data file from the repository by **[read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)**.\n\nWe will use the same DataSet like in previous lab. Therefore next some steps will be the same."
      ]
    },
    {
      "cell_type": "code",
      "id": "f93c5e14-8a1b-451a-b4c9-67c3e0c1cc2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05QLEN/clean_df_new.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2173d477-a115-4c10-b8bb-3fbc7f63b248",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now let's look at our DataSet."
      ]
    },
    {
      "cell_type": "code",
      "id": "33b7e9b1-9e27-440d-8b42-4b310b0b0233",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b16f189c-73e3-4475-9729-eea568a33d38",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Data investigation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e013db-4c52-4905-bff0-d0362cc19539",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's study the DataSet. As you can see the DataSet consist 368 rows × 47 columns. The DataSet consist information of different types. We should be sure that python recognized data types in right way. To do this we should use **[pandas.info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info)**."
      ]
    },
    {
      "cell_type": "code",
      "id": "64b522cf-28ba-49b0-baff-826af3b17223",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769e39e1-aa73-462f-8a2e-72a6875a5e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details>\n<summary><b>Click to see attribute information</b></summary>\n    \nInput features (column names):\n\n1. `Age Group` - patient age divided by groups (categorical)\n2. `Marital Status` - married or single (categorical)\n3. `Lifestyle` - does the patient have healthy lifestyle (boolean)\n4. `Sleep` - does the patient sleep enough?(boolean)\n5. `Category` paid or free treatment (categorical)\n6. `Depression` - does patient feel depressed? (boolean)\n7. `Hyperlipidemia` - an excess of lipids or fats in your blood (boolean)\n8. `Smoking` - does the patient smoke? (boolean)\n9. `Diabetes` - does the patient have diabetes? (binary)\n10. `HTN` - hypertension, also known as high blood pressure (boolean)\n11. `Allergies` - does the patient have allergies? (boolean)\n12. `BP` - blood pressure (float, normalized)\n13. `Thrombolysis` - uses medications or a minimally invasive procedure to break up blood clots and prevent new clots from forming (binary)\n14. `BGR` - blood glucose level (int)\n15. `CPK` - creatine phosphokinase level (int)\n16. `ESR` - erythrocyte sedimentation rate (int)\n17. `WBC` - white blood cells, also known as leukocytes (int)\n18. `RBC` - red blood cells, also known as erythrocytes (float)\n19. `Hemoglobin` - hemoglobin level (float)\n20. `MCH` - mean corpuscular hemoglobin or the average amount in each of red blood cells of a hemoglobin (float)\n21. `MCHC` - mean corpuscular hemoglobin concentration (float)\n22. `PlateletCount` - count of platelets or thrombocytes (int)\n23. `Lymphocyte` - share of lymphocytes in blood (float)\n24. `Monocyte` -  share of monocytes in blood (float)\n25. `Eosinophil` - count of eosinophils (int)\n26. `Others` - other diseases, that weren't mentioned (categorical)\n27. `Diagnosis` - what is the patient's diagnosis? (float)\n28. `Hypersensitivity` - does the patient have hypersensitivity? (boolean)\n29. `Chest pain type` - patient's chest pain stage (int)\n30. `Resting BP` - resting blood pressure (float)\n31. `Serum cholesterol` - amount of total cholesterol in their blood (float)\n32. `FBS` - fasting blood sugar > 120 mg/dl (binary)\n33. `Resting electrocardiographic` - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy) (int)\n34. `Max heart rate` - patient's maximum heart rate achieved (int)\n35. `Angina` - does the patient have exercise induced angina (binary)\n36. `ST depression` - ST depression induced by exercise relative to rest (float)\n37. `Slope` - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) (int)\n38. `Vessels num` - number of major vessels (0-3) colored by flourosopy (int)\n39. `Thal` - 3 = normal; 6 = fixed defect; 7 = reversable defect (int)\n40. `Num` -  diagnosis of heart disease (angiographic disease status) (int)\n41. `Streptokinase` - used to dissolve blood clots that have formed in the blood vessels. Does the patient take it? (binary)\n42. `SK React` - what is the reaction from streptokinase (categorical)\n43. `Follow up` - number of patient's visiting time (int)\n44. `Max heart rate-binned` - patient's maximum heart rate achieved - binned (from Lab2) (categorical)\n45. `Gender-male` - is the patient male (from Lab2)? (binary)\n46. `Locality-urban` - is the patient's locality urban (from Lab2)? (binary)\n\nOutput feature (desired target):\n\n47. `Mortality` - did the patient die of heart failure? (binary)\n    \n    </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac59a66e-c583-4dc0-8a26-5c29c50ef950",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Pipeline Classification <a id=\"pipeline\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "643668d5-5655-44e9-a492-db6773f8fcf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "### LogisticRegression <a id=\"logistic\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8c719e-a088-4b02-8557-a330fd96cd81",
      "metadata": {},
      "outputs": [],
      "source": [
        "Before classification, the dataset must be divided into input and target factors."
      ]
    },
    {
      "cell_type": "code",
      "id": "1d61a2e8-4c14-4f7d-9ac7-39611670c49e",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = df.drop(columns = ['Mortality'])"
      ]
    },
    {
      "cell_type": "code",
      "id": "40b50e9f-a3a2-4c23-9c6c-6c9cc795fff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = df['Mortality']"
      ]
    },
    {
      "cell_type": "code",
      "id": "0b902a51-1481-47d9-af44-8ab81d219d82",
      "metadata": {},
      "outputs": [],
      "source": [
        "x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0293cd-358e-405c-baa2-d217e1663eaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can see the input data set consists from 46 columns.\n\nAs you can see, 7 columns are objects, and all others are numerical and boolean. To make classification, all numerical fields must be normalized and categorical fields must be digitized. This can be automated using the **[sklearn.preprocessing.OrdinalEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** and **[sklearn. preprocessing.StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)**.\n\nSince the machine learning process consists of several steps, each of which has the function `fit`,` predict` and etc, we can combine all these stages into one block using `Pipeline` (**[sklearn.pipeline.make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)**), **[sklearn.compose.make_column_transformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)** and visualize it with: **[sklearn.set_config()](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html)**."
      ]
    },
    {
      "cell_type": "code",
      "id": "a46ec960-cc5a-4f54-973c-e6a073184b6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_cat = list(x.select_dtypes(include=['object']).columns)\ncol_num = list(x.select_dtypes(include=['float', 'int', 'bool']).columns)"
      ]
    },
    {
      "cell_type": "code",
      "id": "389dff2d-3f4a-4b49-b175-a5043cd22676",
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = make_column_transformer((OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),col_cat),\n                                (StandardScaler(),col_num),\n                                remainder = 'passthrough')\nset_config(display = 'diagram')\ntrans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6a64ce8-a435-4620-92e2-91df1b3ff6e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "Next we must separate DataSets for train and test DataSets for calculate accuracy of models. To do this we can use **[sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)**. Let's separate DataSets in 0.3 proportion train/test"
      ]
    },
    {
      "cell_type": "code",
      "id": "365449ea-a7de-4586-8eba-87e541759b1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "id": "1928c5e6-41c4-4249-9b84-9dd1f5f162f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "id": "68a515c4-ea0a-4cb5-9a52-fbdadf230b37",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174a11e1-6f31-4eaa-b297-3a75f620c2bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "Nowe let's create a logistic regression model (**[sklearn.linear_model.LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)**) and add it to our `Pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "id": "6e3151ba-9dbb-4eb2-8564-2d22227d174b",
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\npipe_lr = make_pipeline(trans ,lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccdfe89-f8dc-46dd-adb6-8b0506a81429",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's fit our model."
      ]
    },
    {
      "cell_type": "code",
      "id": "c63ee1f3-b852-4afa-bb91-5487b4424c38",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_lr.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d38e70e-bf18-4734-a831-35673dff90cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Cross-validation <a id=\"cross_val\"></a>\nCross-validation is a resampling technique that involves dividing the data into multiple subsets, allowing the model to be tested and trained on different iterations using different portions of the data. It is commonly employed in scenarios where the objective is prediction, aiming to estimate the performance of a predictive model in real-world applications. The most straightforward approach to implement cross-validation is by utilizing the `cross_val_score` helper function, which takes the estimator (model) and the dataset as input, simplifying the process."
      ]
    },
    {
      "cell_type": "code",
      "id": "93b96b2b-b7cc-4ea8-a4a7-23762881265f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Rcross = cross_val_score(pipe_lr, x, y, cv=4)\nprint(Rcross)\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e334cc15-066a-409a-bfc6-83826e346695",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's use `cros_val_predict` to generate cross-validated estimates for each input data point."
      ]
    },
    {
      "cell_type": "code",
      "id": "c625162f-c4c1-4d0d-bb72-bde4cdc953bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = cross_val_predict(pipe_lr, x, y,cv=4)\nyhat[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b0e259-384e-40e4-9e5d-39916071916a",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Accuracy <a id=\"accuracy\"></a>\nLet's calculate accuracy of this pipeline."
      ]
    },
    {
      "cell_type": "code",
      "id": "bb127d77-729a-438d-9242-3b8c77f8a55e",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_train = pipe_lr.score(x_train, y_train)\nscores_test = pipe_lr.score(x_test, y_test)\nprint('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112be843-13dd-4d51-a661-432828e8cdd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's evaluate the correctness of the classification with: **[sklearn.metrics.plot_confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)** and convince of these conclusions."
      ]
    },
    {
      "cell_type": "code",
      "id": "2fd86768-5b9e-4463-8370-894c5755c4ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(pipe_lr, x_test, y_test, cmap=plt.cm.Blues_r)\n\nplt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1688ed-bd8a-42c5-a58e-fab2435374fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see from the table, our model predicts patient's mortality very well. At the same time, errors in the classification of patients are also not too big. The correct forecast is 101 patients. In 8 cases when the patient actually died of heart failure, the model shows that the patient will survive. Conversely, in the 2 cases, our model predicts that the patient will die of heart failure, but in fact he will survive. That is, the error is great for those patient who really has a risk to die.\n\nThe `Recall` metric is used to assess the accuracy of only purchased goods: **[sklearn.metrics.recall_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "44609958-abcc-43e1-8219-c7a0a56e14c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_train = recall_score(y_train, pipe_lr.predict(x_train))\nscores_test = recall_score(y_test, pipe_lr.predict(x_test))\nprint('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd05db3-b783-4391-b9b0-45b01bbc5cbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "As can be seen from this metric, the accuracy is lower. Moreover, the accuracy of the training and test data are approximately the same. This means that in order to increase this metric of accuracy, it is necessary to increase the training sample. Let's analyze it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c9fc88a-7aa1-47ea-991d-2baca0fbf88f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3. Over-sampling problem <a id=\"over_samp\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f6c429-a736-4814-ae94-fcb902527d38",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's analyze our target column `Mortality` (**[seaborn.countplot()](https://seaborn.pydata.org/generated/seaborn.countplot.html)**):"
      ]
    },
    {
      "cell_type": "code",
      "id": "9819bd83-c958-41d9-b067-dfd083de9516",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.countplot(x = y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "777efc01-9379-4ba0-8295-90bd06fadc7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, the number of cases, when the patients survive is much greater than the number of deaths. To balance the data set, we can use a special function: **[imblearn.over_sampling.RandomOverSampler()](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html)**:"
      ]
    },
    {
      "cell_type": "code",
      "id": "d7ea72db-22b2-4045-8557-fc8da03078f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "ROS = RandomOverSampler()\no_x, o_y = ROS.fit_resample(x,y)\nsns.countplot(x = o_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0325e0a9-de25-4bba-a9b4-0787cd51ce09",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now, as we can see, these numbers are equal. Let's add this function to our `Pipeline`, fit the model and recalculate the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "id": "55c6e43f-8a87-45e4-ae72-396576a99d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_s_lr = make_pipeline(trans, ROS, lr)\npipe_s_lr"
      ]
    },
    {
      "cell_type": "code",
      "id": "73f8274f-52b4-410d-8d9b-dfa2c7f44917",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_s_lr.fit(x_train,y_train)\nscores_train = recall_score(y_train, pipe_s_lr.predict(x_train))\nscores_test = recall_score(y_test, pipe_s_lr.predict(x_test))\nprint('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2820c0e-f3b1-44ea-a87c-a08c2121c83f",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, balancing the dataset has led to a sharp increase in the accuracy of the `Recall` metric.\n\nLet's analyze the errors of the model."
      ]
    },
    {
      "cell_type": "code",
      "id": "b5e8247c-1164-40a9-b336-5ba27b831e06",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(pipe_s_lr, x_test, y_test, cmap=plt.cm.Blues_r)\nplt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f716a6a-bdf7-4c91-b493-942d1280d6c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "As can be seen, the number of erroneous predictions about the patient, who will die, has decreased significantly. However, the error is high when the model predicts mortality rate of patients, who survived. The metric `Precision` is used to assess this accuracy.\n\nTo further increase the `Recall` metric, it is necessary to change the model, because the accuracy of logistic regression on unknown data is about the same as on known data, and therefore it can no longer fit better."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7a22de-948a-4eb3-a6e3-f7b1af08aabd",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question  #1: </h1>\n\n<b>Calculate cross validation score of new pipeline</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "784b9955-f559-4013-86b9-be2f63603233",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f0f299f-d317-4521-aa8c-831f1ba9ca3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nRcross = cross_val_score(pipe_s_lr, x, y, cv=4)\nprint(Rcross)\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9689df7-3a69-42dc-adee-2e7be448ec04",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question  #2: </h1>\n\n<b>Predict the output using cross validation and new pipeline</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "27bac981-3d1a-4067-9e9c-f1cbd592cff0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b0c1402-c090-4d8c-b208-3f7e274dda1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nyhat = cross_val_predict(pipe_s_lr, x, y,cv=4)\nyhat[0:10]\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d0eed5-e7c6-470a-bfd3-5af081362c91",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4. Ensemble of classifiers <a id=\"ensemble\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65921b93-e510-467e-a3e5-e7e032ca842d",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's test other classifiers and compare the results.\nWe will test:\n* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)\n* [Linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html?highlight=linear%20svm#sklearn.svm.LinearSVR)\n* [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)\n* [Extra Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html)\n* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n* [Multi-layer Perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=mlpclassifier#sklearn.neural_network.MLPClassifier)\n* [Ada Boost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier)\n* [Gradient Boosting for classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n* [Bagging classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1cecf7-1f47-43be-af16-7064f6a0c296",
      "metadata": {},
      "outputs": [],
      "source": [
        "In addition, different classifiers may err in different situations. Therefore, to compensate for each other's mistakes, it is necessary to use model ensembles by Voting Classifier.\n\nA **[Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)** a machine learning model that combines the predictions of multiple models in an ensemble to make a final prediction. The concept behind it is to create a single model that leverages the collective knowledge of multiple models, rather than using separate models individually and evaluating their accuracies. It works by aggregating the results from each classifier and selecting the output class based on either hard or soft voting.\n\n- In **Hard Voting**, the predicted output class is determined by majority voting, where the class with the highest number of votes across the classifiers is chosen as the final prediction. For example, if three classifiers predict the output classes as (A, A, B), the majority predicts A, resulting in A being the final prediction.\n\n- In **Soft Voting**, the output class is determined based on the averaged probabilities assigned to each class. The probabilities for each class are averaged across the classifiers, and the class with the highest average probability is selected as the final prediction. For instance, if three models provide prediction probabilities for class A as (0.30, 0.47, 0.53) and for class B as (0.20, 0.32, 0.40), the average probability for class A is 0.4333 and for class B is 0.3067. In this case, class A would be chosen as the final prediction due to its higher averaged probability.\n\nOverall, a Voting Classifier combines the strengths of multiple models to enhance prediction accuracy by leveraging the collective decision-making power of the ensemble."
      ]
    },
    {
      "cell_type": "code",
      "id": "008e1184-0dce-40bc-b2c7-a39575955462",
      "metadata": {},
      "outputs": [],
      "source": [
        "names = [\"Logistic Regression\", \"Linear SVM\",\n         \"Decision Tree\", \"Extra Tree\", \"Random Forest\", \"Neural Net\",\n         \"AdaBoost\", \"GradientBoostingClassifier\", \"BaggingClassifier\", \"VotingClassifier\"]\n\nclassifiers = [\n    LogisticRegression(),\n    SVC(kernel=\"linear\", C=0.025),\n    DecisionTreeClassifier(max_depth=5),\n    ExtraTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(n_estimators=100, random_state=0),\n    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),\n    BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)]\n\nest = [(str(est), est) for est in classifiers]\n\neclf = [VotingClassifier(\n     estimators=est,\n     voting='hard')]\nclassifiers += eclf\nscores_train = []\nscores_test = []\nscores_train_s = []\nscores_test_s = []\n\nfor name, classif in zip(names, classifiers):\n    print(name,'fitting.....')\n    clf = make_pipeline(trans, classif)\n    clf.fit(x_train,y_train)\n    score_train = recall_score(y_train, clf.predict(x_train))\n    score_test = recall_score(y_test, clf.predict(x_test))    \n    scores_train.append(score_train)\n    scores_test.append(score_test)\n    \n    clf_s = make_pipeline(trans, ROS, classif)\n    clf_s.fit(x_train,y_train)\n    score_train_s = recall_score(y_train, clf_s.predict(x_train))\n    score_test_s = recall_score(y_test, clf_s.predict(x_test))    \n    scores_train_s.append(score_train_s)\n    scores_test_s.append(score_test_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "267c485f-4e28-4d69-915e-aec5dc583499",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's compare the accuracy of classifiers for balanced and unbalanced data sets."
      ]
    },
    {
      "cell_type": "code",
      "id": "088cc228-76cd-4490-8b0f-0ef048ab1abf",
      "metadata": {},
      "outputs": [],
      "source": [
        "res = pd.DataFrame(index = names)\nres['Train'] = np.array(scores_train)\nres['Test'] = np.array(scores_test)\nres['Train Over Sampler'] = np.array(scores_train_s)\nres['Test Over Sampler'] = np.array(scores_test_s)\n\nres.index.name = \"Classifier accuracy\"\nres\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "f9f3b6e0-7717-4dbf-9564-020d6aa96c95",
      "metadata": {},
      "outputs": [],
      "source": [
        "res[['Test', 'Test Over Sampler']].plot(kind=\"barh\", figsize=(10,10))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f075c9d-08b0-48c2-a2c4-73ecb3b1ea6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, the balanced data set leads to a sharp increase in accuracy in all classifiers. It can also be seen that the most accurate models were GradientBoostingClassifier, AdaBoost and Neural Net. The ensemble of models showed perfect accuracy.\n\nLet's display the last classifier:"
      ]
    },
    {
      "cell_type": "code",
      "id": "150bd46f-7069-4add-b9ec-6e54d8bd7ad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03d7dd8-3af1-45b0-831b-c7df951ddbf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question  #3: </h1>\n\n<b>Choose 3 best classifiers and make an ensemble based on them</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "acd3b482-1b9d-42f4-8c0c-1a7bba85d4b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b1d9ba-6cd2-48cd-b14b-5b1f2c51f95e",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nclassifiers = [\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(n_estimators=100, random_state=0),\n    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)]\n\nest = [(str(est), est) for est in classifiers]\n\neclf = VotingClassifier(\n     estimators=est,\n     voting='hard')\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8717707c-6aa8-444b-99e3-b05b9622be36",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question  #4: </h1>\n\n<b>Add it to `Pipeline` (use Over Sampler) and fit it</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "00ccbac3-bae8-4f46-8658-5d2df6435376",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80d0df2d-fb01-4800-bfc4-a59999bc761f",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\npipe_eclf = make_pipeline(trans, eclf)\npipe_eclf.fit(x_train,y_train)\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f65e1ab-2c1f-464d-a3d3-46cf65e46304",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question  #5: </h1>\n\n<b>Calculate the accuracy of this ensemble</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "a86c39ad-c43c-4f7e-a50e-e31a435c15bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c2210f-c16f-4626-8985-80e331ca51de",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nscore_train = recall_score(y_train, pipe_eclf.predict(x_train))\nscore_test = recall_score(y_test, pipe_eclf.predict(x_test)) \nprint('Training DataSet accuracy: {: .1%}'.format(score_train), 'Test DataSet accuracy: {: .1%}'.format(score_test))\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "301110be-e03f-4eb9-96ca-becae2600bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af421f82-ee8f-4211-bb96-a7dad93b8717",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab we studied how to build training and test data sets and how to fit different classifiers, evaluate their accuracy and analyze errors.\nWe also studied how to join them together in an ensemble and create a model based on Pipeline.\nWe compared the accuracy of different classifiers and their ensemble and showed how they can be used in medicine on the example of patient classification.\n\nThe accuracy of the VotingClassifier was about 100%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76702955-9ad4-487b-834a-f389aa2e80ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Thank you for completing this lab!\n\n## Author\n\n<a href=\"https://author.skills.network/instructors/bohdan_kuno\">Bohdan Kuno</a>\n\n### Other Contributors\n\n<a href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2\">Prof. Yaroslav Vyklyuk, DrSc, PhD</a>\n\n<a href=\"https://author.skills.network/instructors/nataliya_boyko\">Ass. Prof. Nataliya Boyko, PhD</a>\n\n## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                         |\n| ----------------- | ------- | ---------- | ---------------------------------------------------------- |\n|2023-03-25|01|Bohdan Kuno|Lab created|\n\n\n<hr>\n\n## <h3 align=\"center\"> © IBM Corporation 2023. All rights reserved. <h3/>\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}