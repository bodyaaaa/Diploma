{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53bcea12-47f7-46d3-be8d-3ec8ca1bd592",
      "metadata": {},
      "outputs": [],
      "source": [
        "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n</center>\n\n# **Heart Failure Prediction**\n\n# Lab 4. Model Development\n\n# Abstract\nIn this lab, you will focus on preliminary data processing techniques. You will learn how to modify data types, normalize and process categorical data, and apply various feature selection methods. Additionally, you will explore working with different classifiers and gain insights into visualizing decision trees. By the end of the lab, you will have gained the knowledge and skills required to predict the likelihood of a patient experiencing heart failure based on different models.\n\nEstimated time needed: **30** minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9217d05f-0829-4351-b5a7-fb8923692d53",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Objectives\n\n1. preprocess (normalize and transform categorical data) and create DataSet\n2. features selection\n3. make classification of patients\n4. visualize the decision tree of a classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5986268-d8ce-42f9-837c-8cc3128d91d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "<h2>Table of Contents</h2>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n<ol>\n    <li><a href=\"#prep\">Data preparation</a></li>\n    <li><a href=\"#select\">Features selection</a></li>\n    <li><a href=\"#classif\">Classification models</a>\n        <ul>\n        <li><a href=\"#decision\">Decision tree</a>\n        <li><a href=\"#extra\">Extra Trees Classifier</a>\n        <li><a href=\"#logistic\">Logistic regression</a>\n        </ul>\n    </li>\n    <li><a href=\"#visualization\">Visualization of the decision tree</a></li>\n</ol>\n\n</div>\n\n<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa9419d-4e81-4422-a3ea-ec27163653eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Materials and Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c22bf295-87e9-4d3c-8b70-518806474b48",
      "metadata": {},
      "outputs": [],
      "source": [
        "The data that we are going to use for this is a subset of an open source The Heart Failure Prediction Dataset. https://www.kaggle.com/datasets/asgharalikhan/mortality-rate-heart-patient-pakistan-hospital.\n\n> This dataset is publicly available for research. \nThe dataset consists of comprehensive records of heart patients, making it accessible for data scientists from various regions worldwide to work with. The data is collected from the Institute of Cardiology, a hospital located in Faisalabad, Pakistan.\n\nIn this lesson, we will try to give answers to a set of questions that may be relevant when analyzing heart failure data:\n\n1. What are the most useful Python libraries for classification analysis?\n2. How to transform category data?\n3. How to create DataSet?\n4. How to do features selection?\n5. How to make, fit, and visualize a classification model?\n\nIn addition, we will make the conclusions from the obtained results of our classification analysis to predict the mortality rate of patients with heart disease."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0fa351-17c1-473e-9a43-dd6b44840146",
      "metadata": {},
      "outputs": [],
      "source": [
        "[Scikit-learn](https://scikit-learn.org/stable/), previously known as scikits.learn and commonly referred to as sklearn, is an open-source machine learning library for Python. It provides a wide range of algorithms for tasks such as classification, regression, and clustering. These include popular methods like support vector machines, random forests, gradient boosting, k-means, and DBSCAN. Scikit-learn is designed to seamlessly integrate with other Python libraries for numerical and scientific computing, such as NumPy and SciPy."
      ]
    },
    {
      "cell_type": "code",
      "id": "86c880a4-f17c-47ac-a49a-4aa707fbcda3",
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda install --yes scikit-learn==0.24.2\n!conda install --yes python-graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c2cc5c3-ce69-4d71-933f-db2221f26a6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24b14718-2c43-4a3a-95a3-8e57d64cc726",
      "metadata": {},
      "outputs": [],
      "source": [
        "Import the libraries necessary to use in this lab. We can add some aliases to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "99526786-0870-4ce5-8309-180f3db592bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (8, 6)\n# Data transformation\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n# Features Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, mutual_info_classif\n# Classificators\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import tree\n# warnings deactivate\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8cf9eb7-c35b-416d-bf70-627e93551c91",
      "metadata": {},
      "outputs": [],
      "source": [
        "Further, specify the value of the `precision` parameter equal to 2 to display two decimal signs (instead of 6 as default)."
      ]
    },
    {
      "cell_type": "code",
      "id": "87d3fb67-a443-4924-a873-98e1da1b0aaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad178cc-e180-48da-9890-f47139fa96c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89de0f9a-4f2d-48d3-af3a-c11061e5b499",
      "metadata": {},
      "outputs": [],
      "source": [
        "We will use the same DataSet that we have saved in previous labs."
      ]
    },
    {
      "cell_type": "code",
      "id": "5693f1a9-1ce6-4243-82a5-6f37fe60a219",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0MI1EN/clean_df_new.csv')\ndf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "id": "58a52275-8dfe-4ebb-a6d3-da94c1472b04",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d3662e-15a0-460f-9176-7038230832c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see DataSet consists of 47 columns. The target column is 'Mortality'. Also DataSet consists of 368 rows. In previous labs we investigated these columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d85f177-7379-4a85-abfe-42471945cafe",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details>\n<summary><b>Click to see attribute information</b></summary>\n\nInput features (column names):\n\n1. `Age Group` - patient age divided by groups (categorical)\n2. `Marital Status` - married or single (categorical)\n3. `Lifestyle` - does the patient have a healthy lifestyle (boolean)\n4. `Sleep` - does the patient sleep enough?(boolean)\n5. `Category` paid or free treatment (categorical)\n6. `Depression` - does a patient feel depressed? (boolean)\n7. `Hyperlipidemia` - an excess of lipids or fats in your blood (boolean)\n8. `Smoking` - does the patient smoke? (boolean)\n9. `Diabetes` - does the patient have diabetes? (binary) \n10. `HTN` - hypertension, also known as high blood pressure (boolean) \n11. `Allergies` - does the patient have allergies? (boolean)\n12. `BP` - blood pressure (float, normalized) \n13. `Thrombolysis` - uses medications or a minimally invasive procedure to break up blood clots and prevent new clots from forming (binary) \n14. `BGR` - blood glucose level (int) \n15. `CPK` - creatine phosphokinase level (int)\n16. `ESR` - erythrocyte sedimentation rate (int) \n17. `WBC` - white blood cells, also known as leukocytes (int) \n18. `RBC` - red blood cells, also known as erythrocytes (float) \n19. `Hemoglobin` - hemoglobin level (float) \n20. `MCH` - mean corpuscular hemoglobin or the average amount in each of red blood cells of hemoglobin (float)\n21. `MCHC` - mean corpuscular hemoglobin concentration (float)\n22. `PlateletCount` - count of platelets or thrombocytes (int)\n23. `Lymphocyte` - share of lymphocytes in blood (float)\n24. `Monocyte` -  share of monocytes in blood (float)\n25. `Eosinophil` - count of eosinophils (int)\n26. `Others` - other diseases, that weren't mentioned (categorical)\n27. `Diagnosis` - what is the patient's diagnosis? (float)\n28. `Hypersensitivity` - does the patient have hypersensitivity? (boolean)\n29. `Chest pain type` - patient's chest pain stage (int)\n30. `Resting BP` - resting blood pressure (float)\n31. `Serum cholesterol` - amount of total cholesterol in their blood (float)\n32. `FBS` - fasting blood sugar > 120 mg/dl (binary)\n33. `Resting electrocardiographic` - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy) (int)\n34. `Max heart rate` - patient's maximum heart rate achieved (int)\n35. `Angina` - does the patient have exercise induced angina (binary)\n36. `ST depression` - ST depression induced by exercise relative to rest (float)\n37. `Slope` - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) (int)\n38. `Vessels num` - number of major vessels (0-3) colored by flourosopy (int)\n39. `Thal` - 3 = normal; 6 = fixed defect; 7 = reversable defect (int)\n40. `Num` -  diagnosis of heart disease (angiographic disease status) (int)\n41. `Streptokinase` - used to dissolve blood clots that have formed in the blood vessels. Does the patient take it? (binary)\n42. `SK React` - what is the reaction from streptokinase (categorical)\n43. `Follow up` - number of patient's visiting time (int)\n44. `Max heart rate-binned` - patient's maximum heart rate achieved - binned (from Lab2) (categorical)\n45. `Gender-male` - is the patient male (from Lab2)? (binary)\n46. `Locality-urban` - is the patient's locality urban (from Lab2)? (binary)\n\nOutput feature (desired target):\n\n47. `Mortality` - did the patient die of heart failure? (binary)\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67804d77-1c79-419e-ab79-4526879e6f5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Our goal is to create a model for predicting mortality caused by Heart Failure. To do this we must analyze and prepare data for such type of model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d61181f-4ba0-4827-b770-73a52a2d6fe7",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Data preparation <a id=\"prep\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5d374b-34a4-4665-8b96-0effe5d573a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Data transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70b8371-952a-4113-a3f1-71c37ee54bd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "First of all we should investigate how pandas recognized types of features"
      ]
    },
    {
      "cell_type": "code",
      "id": "70c8c705-d3e9-43c8-a910-d5715d11721b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06e1c374-54b1-428f-9fc6-4d09fa4abb91",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see all categorical features were recognized like object. We must change their type to \"categorical\". "
      ]
    },
    {
      "cell_type": "code",
      "id": "56047697-5b72-4abf-ac50-9a76e04acb51",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_cat = list(df.select_dtypes(include=['object']).columns)\ncol_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b28b3d-3495-4709-a0a4-fe4627192c2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's look at the dataset size."
      ]
    },
    {
      "cell_type": "code",
      "id": "fec1fa66-8d78-4ab7-bc41-0d039296f703",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.loc[:, col_cat] = df[col_cat].astype('category')\ndf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c9c4e4-59af-4d03-b649-b07a198f21f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "To see the unique values of the exact feature (column) we can use:"
      ]
    },
    {
      "cell_type": "code",
      "id": "93323a49-3674-4a9c-af58-0de1254085dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Age Group'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8636c0-6674-463b-b03a-7d01b2097924",
      "metadata": {},
      "outputs": [],
      "source": [
        "As was signed earlier the dataset contains 368 objects (rows), for each of which 47 features are set (columns), including 1 target feature (y). 7 features are categorical. These data types of values cannot be used for classification. We must transform it to int or float.\nTo do this we can use **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)** and **[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)**. These functions can encode categorical features as an integer array.\n\nFirst of all we separate DataSet on input and output(target) DataSets"
      ]
    },
    {
      "cell_type": "code",
      "id": "3b44c56a-5993-4fa2-8da1-b5d6ae50dafd",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(['Mortality'], axis=1)  #input columns\ny = df['Mortality']   #target column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01946683-32b8-48df-86a0-ff1aaecb62ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Encoding and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5ed225-7607-4e56-818f-3db677033858",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's see how many unique values our target column has."
      ]
    },
    {
      "cell_type": "code",
      "id": "a050b1d0-6bdd-4d56-9700-e9b47cd1f703",
      "metadata": {},
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accce7ed-0d24-4f50-9ee4-ec3b65108910",
      "metadata": {},
      "outputs": [],
      "source": [
        "We have 288 0 values and 80 1 values, which is acceptable for predicting our target column."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "975b5a05-f18f-4691-9cd9-155c4ba66b03",
      "metadata": {},
      "outputs": [],
      "source": [
        "Then create a list of categorical fields and transform their values into int arrays:"
      ]
    },
    {
      "cell_type": "code",
      "id": "077627f3-6240-4730-a8ac-db6f4dbe70b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_cat = list(X.select_dtypes(include=['category']).columns)\noe = OrdinalEncoder()\noe.fit(X[col_cat])\nX_cat_enc = oe.transform(X[col_cat])"
      ]
    },
    {
      "cell_type": "code",
      "id": "85ebe93e-d738-4fde-b526-ab0772956225",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_cat_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da01aa08-9d53-4bf2-891d-97471b416701",
      "metadata": {},
      "outputs": [],
      "source": [
        "Then we must transform arrays back into DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "id": "a1921592-0f18-4aa0-a360-32a7fddcb3dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_cat_enc = pd.DataFrame(X_cat_enc)\nX_cat_enc.columns = col_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb02f0c-398a-4afc-8426-173cf867f6cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "Numerical fields can have a different scale and can consist of negative values. These will lead to round mistakes and exceptions for some AI methods. To avoid it these features must be normalized.\n\nLet's create a list of numerical fields and normalize it using by **[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "64dcc5be-ea99-4193-8c5c-879ae6548cab",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_num = list(X.select_dtypes(include=['float', 'int', 'bool']).columns)\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_num_enc = scaler.fit_transform(X[col_num])"
      ]
    },
    {
      "cell_type": "code",
      "id": "23472fbb-54e6-43d1-b527-1b360a6a1205",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_num_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2550d69-a6ea-4073-b6ff-cce98d77d872",
      "metadata": {},
      "outputs": [],
      "source": [
        "Like in the previous case transform back obtained arrays into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "id": "9517f58e-a312-489d-8acb-210891ac0b34",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_num_enc = pd.DataFrame(X_num_enc)\nX_num_enc.columns = col_num\nX_num_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4684a70c-41dc-4328-a859-2fe25fccb2b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "Then we should concatenate these DataFrames in one input DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "id": "943029b5-27e9-4be5-9ae0-37c04a4bb43a",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_enc = pd.concat([X_cat_enc, X_num_enc], axis=1)\nx_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ef004c-85ab-4e9c-9bd0-e052ad87daa9",
      "metadata": {},
      "outputs": [],
      "source": [
        "Our target column is already normalized, so we don't need to encode it.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184ea4c5-acee-4b21-99d0-626b70b5adc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Features selection <a id=\"select\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7244982c-17f1-421e-b4f7-d2736c44e59b",
      "metadata": {},
      "outputs": [],
      "source": [
        "As was signed before input fields consist of 46 features. Of course, some of them are more significant for classification.\n\nThere are two popular feature selection techniques that can be used for categorical input data and a categorical (class) target variable.\n\nThey are:\n\n* Chi-Squared Statistic.\n* Mutual Information Statistic.\n\nLet’s take a closer look at each in turn.\n\nTo do this we can use **[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e861ec6-24c9-45a9-a135-dc6be6177712",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Chi-Squared Statistic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7172bea1-4460-41c4-a66a-81119fc9b278",
      "metadata": {},
      "outputs": [],
      "source": [
        "Pearson's chi-squared statistical hypothesis test serves as an instance of a test used to determine independence between categorical variables.\n\nYou can learn more about this statistical test in the tutorial:\n\n[A Gentle Introduction to the Chi-Squared Test for Machine Learning](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/)\nThe results of this test can be used for feature selection, where those features that are independent of the target variable can be removed from the dataset.\n\nThe scikit-learn machine library provides an implementation of the chi-squared test in the **[chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)** function. This function can be used in a feature selection strategy, such as selecting the top k most relevant features (largest values) via the SelectKBest class.\n\nFor example, we can define the SelectKBest class to use the chi2() function and select all (or most significant) features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30ff7048-4710-42c6-95a1-0e5388f27aa8",
      "metadata": {},
      "outputs": [],
      "source": [
        "Apply SelectKBest class to extract the top 10 best features"
      ]
    },
    {
      "cell_type": "code",
      "id": "0b44ae18-3e02-4749-9a78-0e0ecd239c91",
      "metadata": {},
      "outputs": [],
      "source": [
        "bestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(x_enc, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3bf184-440d-497e-8e49-c18690cc57d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "Concat two dataframes for better visualization "
      ]
    },
    {
      "cell_type": "code",
      "id": "1bba0f67-f4dd-4497-93b8-4690fe74f60e",
      "metadata": {},
      "outputs": [],
      "source": [
        "featureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940435bf-ce28-43db-9816-ac66b44f2be0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Mutual Information Statistic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac726a22-72d0-49e0-b2cf-bf4e60ae19b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "In the field of information theory, mutual information is the utilization of information gain, commonly employed in decision tree construction, for feature selection.\n\nMutual information quantifies the reduction in uncertainty of one variable when a known value of another variable is present. It calculates the dependency between two variables based on their shared information content.\n\n[You can learn more about mutual information in the following tutorial.](https://machinelearningmastery.com/information-gain-and-mutual-information)\n\nThe scikit-learn machine learning library provides an implementation of mutual information for feature selection via the **[mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)** function.\n\nLike chi2(), it can be used in the SelectKBest feature selection strategy (and other strategies)."
      ]
    },
    {
      "cell_type": "code",
      "id": "0ae33e43-cecb-4f8b-a31b-a01ac019a9bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "bestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(x_enc, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a084116f-d1b8-4ed5-8072-7ece6f70c8b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see these 2 functions select different significant features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7e11255-2ca2-40fc-971a-0da1fef88dd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f3753d-b579-46aa-bfd5-4638a3e6a449",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can get the feature importance of each feature of your DataFrame by using the feature importance property of the exact classification model. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant the feature is to your output variable. For example, \nfeature importance is an inbuilt class that comes with **[Tree Based Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)**, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee8b264-4c57-4d8f-9c67-15e68aefefb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create and fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "id": "3bff1730-5c20-412e-8e3a-51092e4de9a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ExtraTreesClassifier()\nmodel.fit(x_enc, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee1dd3fd-d9b1-45b2-9c53-83d2915f799a",
      "metadata": {},
      "outputs": [],
      "source": [
        "Use inbuilt class `feature_importances` of tree based classifiers"
      ]
    },
    {
      "cell_type": "code",
      "id": "b910659e-d51c-49da-9744-e53d9f83fe9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98919b1-9779-41d3-91ba-6b792c3ee540",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's transform it into a `Series` and plot a graph of features' importance for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "id": "955a9827-70f6-4d1b-9abc-68d88ea9d7f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66e10376-3f2c-4572-8871-b56a6e92c2f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can see that for Extra Tree Classifier impotance of features are different than in previous cases. It means that there are not exact rules for features selection. And their impotance strictly depedence on model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c53c67-51fa-4201-a8e4-1e2e09ca85c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n    <h1>Question 1:</h1>\n    <p>Plot graph of 5 least important features</p>\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "4109d3ce-bf10-480c-a4db-3b290b45927f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5867f142-e241-4d7e-b48b-165377e8bc04",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nfeat_importances.nsmallest(5).plot(kind='barh')\nplt.show()\n\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19c0904-06c6-4fcc-a222-ad53d4448db3",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Correlation Matrix with Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23f1481-fb2d-46ed-8c4e-733c63bb6741",
      "metadata": {},
      "outputs": [],
      "source": [
        "Correlation describes the relationship between features in a dataset. It can be positive, indicating that an increase in one feature corresponds to an increase in another, or negative, suggesting that an increase in one feature leads to a decrease in another. By utilizing the seaborn library, we can create a heatmap that visually highlights the most closely related features to another variable. This heatmap enables us to easily identify the strength and direction of correlations within the dataset."
      ]
    },
    {
      "cell_type": "code",
      "id": "2c1de6cb-7a02-4513-9f83-4a653cc52bd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "corrmat = x_enc.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(35,35))\ng=sns.heatmap(x_enc[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4af8093-278b-4233-b823-8cdac76baf28",
      "metadata": {},
      "outputs": [],
      "source": [
        "We have already removed strictly correlated columns in previous lab, thus we can use all these features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120fc682-33b7-46f4-8798-cab232f54423",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Classification models<a id=\"classif\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b613b10f-bb35-4880-8420-9446b9956721",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Decision tree <a id=\"decision\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0902f237-267e-4bb2-bc73-f0f0897bfab0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009e70e0-87b0-417c-8565-8c87074c2749",
      "metadata": {},
      "outputs": [],
      "source": [
        "As shown, the previous methods have high accuracy. However, the biggest drawback is the inability to visualize or justify the decision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260a3df4-84a5-4185-ae92-244eca1ddd17",
      "metadata": {},
      "outputs": [],
      "source": [
        "Decision trees are widely employed in supervised learning for several reasons. They can be utilized for both regression and classification tasks, eliminating the need for feature scaling. Additionally, decision trees offer relatively straightforward interpretability, as they can be visualized. This visualization not only aids in understanding the model but also facilitates communication regarding its functioning. Therefore, it is valuable to learn how to create visualizations based on your model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f25be04-91a2-4c21-947d-e379eb0f31f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "A **[Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)** is a supervised machine learning algorithm that employs a binary tree graph, where each node has two children, to assign a target value to each data sample. The target values are located in the tree's leaves. The sample traverses through nodes, starting from the root node, to reach the leaf node. At each node, a decision is made to determine which descendant node the sample should proceed to. This decision is based on the features of the selected sample. Decision Tree learning involves identifying the optimal rules in each internal tree node based on a chosen metric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80dd9ac7-fc60-4a87-afdb-bb54ffe210af",
      "metadata": {},
      "outputs": [],
      "source": [
        "This method allows also us to calculate the features' importance. Let's calculate them. Choice the best 10 of them. Refit the model and visualize the decision tree."
      ]
    },
    {
      "cell_type": "code",
      "id": "5d7e47a2-fe85-4f03-bd83-721d36a7412c",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dec_tree = DecisionTreeClassifier()\nmodel_dec_tree.fit(x_enc, y)\nyhat = model_dec_tree.predict(x_enc)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc649f8-608d-47a1-97d1-49881dae8540",
      "metadata": {},
      "outputs": [],
      "source": [
        "Create a user function that will calculate the accuracy of the defined classification model:"
      ]
    },
    {
      "cell_type": "code",
      "id": "a92b9974-d7ea-4afd-8d60-789484e2f262",
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_ac(x, y, model):\n    model.fit(x, y)\n    yhat = model.predict(x)\n    accuracy = accuracy_score(y, yhat)\n    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d51ea2-693f-482a-87e7-9183649ef9bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now let's create a user function that will calculate the features' importance of the defined classification model. And let's create a variable, that contains features sorted by importance in descending order."
      ]
    },
    {
      "cell_type": "code",
      "id": "eb374c72-cf78-46f8-add9-b352ba57cc8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_imp(x, y, model):\n    feat_importances = pd.Series(model.feature_importances_, index=x.columns)\n    return feat_importances.sort_values(ascending=False)\nimp = model_imp(x_enc, y, model_dec_tree)\nprint(imp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5380b65b-1668-4544-aafc-5a87c88cc892",
      "metadata": {},
      "outputs": [],
      "source": [
        "Plot graph of feature importances for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "id": "be456f00-fe6a-434b-a93c-919a0d29e243",
      "metadata": {},
      "outputs": [],
      "source": [
        "imp.nlargest(10).plot(kind='barh')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac2e52f-9227-4f57-a04a-d2b1d3a2236d",
      "metadata": {},
      "outputs": [],
      "source": [
        "Build a plot that shows the accuracy of the defined model dependence on the number of input features."
      ]
    },
    {
      "cell_type": "code",
      "id": "b97019b0-6586-468e-8a51-e6c37f1f2df3",
      "metadata": {},
      "outputs": [],
      "source": [
        "col = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(x_enc[col], y, model_dec_tree))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.Series(ac)\nac.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f513b8-7e3b-4de8-800d-ecaebfd06bf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see that 5 features are enough to make 100% accuracy. So let's create a list of these 5 features in order to use them for our next classification models."
      ]
    },
    {
      "cell_type": "code",
      "id": "e981be6a-6161-4796-a362-48f8366c844b",
      "metadata": {},
      "outputs": [],
      "source": [
        "col = imp.nlargest(5).index\ncol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b3bb0d-e726-4304-8eb0-3366151dc458",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's refit the model on most important features"
      ]
    },
    {
      "cell_type": "code",
      "id": "7cdb5241-23d3-476a-bfaf-5e2c0b2133a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_most_imp = x_enc[col]\nmodel_dec_tree.fit(X_most_imp, y)\nyhat = model_dec_tree.predict(X_most_imp)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14d4814-aa6d-48a3-8dac-9171ecada022",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Extra Trees Classifier <a id=\"extra\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bba23a0-eec4-4f04-b496-ac4044fd094d",
      "metadata": {},
      "outputs": [],
      "source": [
        "**[Extra Trees Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)** is an ensemble machine learning algorithm that belongs to the family of decision tree-based classifiers. It is an extension of the Random Forest algorithm, but with some differences in the way it builds and combines decision trees. Extra Trees creates a large number of random decision trees and combines their predictions through voting to make the final classification. It introduces additional randomness in the tree-building process by selecting random features and thresholds at each node, which can lead to improved generalization and robustness against overfitting. Overall, Extra Trees Classifier is known for its simplicity, efficiency, and effectiveness in handling high-dimensional datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d55f7a8-6e1b-4161-8764-715b40896746",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create and fit ExtraTreesClassifier on the most important features and calculate the accuracy of classification:"
      ]
    },
    {
      "cell_type": "code",
      "id": "1f239fec-1a8a-44fe-b612-e6790dd789b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ExtraTreesClassifier()\nmodel.fit(X_most_imp, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c1903f-55c2-44ac-86ff-a5deb267367b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Evaluate the model to obtain predictions"
      ]
    },
    {
      "cell_type": "code",
      "id": "7d8f33eb-7e17-4f90-9c92-55c71043c029",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = model.predict(X_most_imp)\nprint(yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564d3c6c-c53e-434f-ba57-84358d777e0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Evaluate the accuracy: "
      ]
    },
    {
      "cell_type": "code",
      "id": "7c98b506-3811-4038-a7ed-9c5b1b7a9279",
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849a8110-5348-4274-8252-1c008053fa80",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n    <h1>Question 2:</h1>\n    <p>Create the variable, that contains features sorted by importance in descending order for the Extra Tree model (using 5 most important features)</p>\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "e0e93165-c116-4e8c-84ab-739428b84c08",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82ed378-a1d2-484c-8360-38f3e86002d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nimp = model_imp(X_most_imp, y, model)\nprint(imp)\n\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f189da29-0703-42cf-b849-414895665ce0",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n    <h1>Question 3:</h1>\n    <p>Build a plot that shows the accuracy of the Extra Tree model dependence on the numbers of input features (using 5 most important features)</p>\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "786fd7bc-ef40-4983-ae91-53a710b8a71b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b8d38ed-96b2-422b-a646-118a6ea47be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\ncol = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(X_most_imp[col], y, model))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.Series(ac)\nac.plot()\n\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc71eb5f-4003-4a04-a056-3cc391c7150c",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Logistic regression <a id=\"logistic\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b4d276-036e-4eb9-acae-dcb4e949829b",
      "metadata": {},
      "outputs": [],
      "source": [
        "There are many different techniques for scoring features and selecting features based on scores; how do you know which one to use?\n\nA robust approach is to evaluate models using different feature selection methods (and numbers of features) and select the method that results in a model with the best performance.\n\n**[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** serves as a suitable model for evaluating feature selection methods because it can potentially yield improved performance when irrelevant features are eliminated from the model. We will utilize this model in a manner identical to the previous one, following the same approach and methodology."
      ]
    },
    {
      "cell_type": "code",
      "id": "ec2e3c3c-cebd-48f7-836c-17874606b07a",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_most_imp, y)\nyhat = model.predict(X_most_imp)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54576b40-d10a-4f0d-95b4-6676d21a3d72",
      "metadata": {},
      "outputs": [],
      "source": [
        "As we can see, the accuracy of the Logistic Regression model is lower (about 80%)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1680b7-7d9a-42b5-b8a8-0b87f7e1459b",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n    <h1>Question 4:</h1>\n    <p>Calculate the accuracy of the Logistic Regression model using all features.</p>\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "83d1cce3-fb54-4fd7-8ce7-ecddc2b249fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33509c7c-44ac-4201-a8d0-791b22454654",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(x_enc, y)\nyhat = model.predict(x_enc)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))\n\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa92b38-5704-42b2-adcf-77296ea66dff",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Visualization of the decision tree<a id=\"visualization\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb67259-4c47-422d-ab96-3f2acbade387",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's visualize the decision tree.\nThere are some ways to do it. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3685c3-d50c-409b-851c-a915f3dce9bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "### _Text visualization_"
      ]
    },
    {
      "cell_type": "code",
      "id": "7cb008de-04e3-421e-9b53-de8a68f41afa",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_representation = tree.export_text(model_dec_tree)\nprint(text_representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a48972c-2064-445b-bf8f-a1a187aa1311",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can save it into the file:"
      ]
    },
    {
      "cell_type": "code",
      "id": "37d5c33e-8b32-4492-8351-a84da1d594d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f30cab-c2a8-4982-8eb4-3a7701159123",
      "metadata": {},
      "outputs": [],
      "source": [
        "### _Plot tree_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c652e9f-5fcb-4de6-96de-1a94afa12252",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can plot a tree using two different ways:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86541062-f88a-4fed-b4eb-50f71e0742b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "**[plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)** (slow render - this can take some time): \n"
      ]
    },
    {
      "cell_type": "code",
      "id": "4010eb5b-c1e4-4bcb-b090-10282217265a",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model_dec_tree,\n               feature_names = col, \n               filled = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948c43fd-1b1d-4059-b5ff-cc7f02b594c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "This tree is aimed to help the doctor make a decision about whether our patient is supposed to die of heart failure. But it's problematic to make a decision because data is normalized here, so we need to fix that. Let's create a model, where the data isn't normalized. "
      ]
    },
    {
      "cell_type": "code",
      "id": "84e84c0c-8f64-477d-8526-1341400d093b",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_real = DecisionTreeClassifier()\nX_most_imp_real = df[X_most_imp.columns]\n\noe = OrdinalEncoder()\noe.fit(X_most_imp_real[['Age Group']])\nX_most_imp_real['Age Group'] = oe.transform(X_most_imp_real[['Age Group']]) \n\nmodel_real.fit(X_most_imp_real, y)\nyhat = model_real.predict(X_most_imp_real)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c15850-fc89-46b9-a13b-bae5459feb83",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see, we also use `OrdinalEncoder()` to encode categorical columns ('Age Group' in this case) in order to provide correct code execution. We can't make a DecisionTree model using categorical values. So after that, we can fit the model and predict our target column."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be3bcfad-1fd9-4ade-9550-634bc81e0de1",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's plot what we've got"
      ]
    },
    {
      "cell_type": "code",
      "id": "e205414b-42ce-4b95-93c5-0f994b34218c",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model_real,\n               feature_names = col, \n               filled = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e157fd5-f78c-4105-a2e2-a3071993980b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Save the picture"
      ]
    },
    {
      "cell_type": "code",
      "id": "a8a1d6c6-bac3-4233-9a8a-1b816072d3e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig.savefig('decision_tree.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e668b9a0-8d38-45c8-bbc3-27e37b6a25a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now we can see our decision tree is much more understandable. But how can we read column 'Age Group'? In order to deal with this problem, we need to compare numerical values with categorical values. We have 5 categories and 5 numerical values: <br>\n'21-30' - 0, <br>\n'31-40' - 1, <br>\n'41-50' - 2, <br>\n'51-60' - 3, <br>\n'61-70' - 4 <br>\nSo 'Age Group <= 2.5' means that first three categories are correct ('21-30', '31-40', '41-50')<br>\n<h3>How to make a decision</h3>\n<p>If the expression is true, we move to the left, if it's false - to the right. For example, we have a patient and data about him:\n    'Age Group' - '51-60', <br>\n    'Serum cholesterol' - 8.0, <br>\n    'Gender-male' - 0, <br>\n    'Hemoglobin' - 8.0, <br>\n    'Diabetes' - 1 <br>\nAge Group <= 2.5' - false; Gender-male <= 0.5 - true; Serum cholesterol <= 7.913 - false; Hemoglobin <= 8.006 - true. The result is true, our patient will die of heart failure.</p>\n<p>Also, as another example, we can take real data from our DataSet. Let's take a patient with index 0 and check whether the result will be the same.\n    'Age Group' - '41-50', <br>\n    'Serum cholesterol' - 10-58, <br>\n    'Gender-male' - 0, <br>\n    'Hemoglobin' - 7.2, <br>\n    'Diabetes' - 1 <br>\nAge Group <= 2.5' - true; Hemoglobin <= 9.992 - true; Diabetes <= 0.5 - false; Hemoglobin <= 8.192 - true; Hemoglobin <= 6.951 - false.  The result is false, this patient won't die of heart failure. Moreover, in the DataSet 'Mortality' of the patient with a 0 index is 0. So, the result is successful and we learned how to predict the mortality of patients using a decision tree. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f1c317-6e1b-4db6-b5c3-84debff84c7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "Or you can use the **[python-graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html)** library. This is a faster function"
      ]
    },
    {
      "cell_type": "code",
      "id": "4267a123-6e26-4395-a353-827410ac92ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "dot_data = tree.export_graphviz(model_real,\n               feature_names = col,\n                                filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc54a21-4862-4445-b934-21e56a2f4904",
      "metadata": {},
      "outputs": [],
      "source": [
        "After creation you can draw the graph"
      ]
    },
    {
      "cell_type": "code",
      "id": "e60a7172-0ef8-4bb0-be87-4688dc1df88c",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = graphviz.Source(dot_data, format=\"png\") \ngraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b313a757-caff-4f8a-9b84-22e42acfeebe",
      "metadata": {},
      "outputs": [],
      "source": [
        "And render it into the file:"
      ]
    },
    {
      "cell_type": "code",
      "id": "1cf09e0c-d50b-4eab-800b-a64b870e2022",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph.render(\"decision_tree_graphivz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90793ada-044a-4f22-90ae-436aac1f5374",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d840d7-f432-424f-8089-412255f45f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab, we learned to do preliminary data processing. In particular, change data types, and normalize and process categorical data. It was shown how to make feature selections by different methods. Shows how to work with different classifiers. It was also shown how to visualize a decision tree. As a result of the lab, it was shown how on the basis of a statistical database predict if the patient will die of heart failure.\n\nThe accuracy of the Decision Tree and Extra Tree classifiers was 100%. And the accuracy of Logistic Regression is about 80%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3eceb0e-0b3a-4a41-8cd9-136ea3502d96",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Thank you for completing this lab!\n\n## Author\n\n<a href=\"https://author.skills.network/instructors/bohdan_kuno\">Bohdan Kuno</a>\n\n### Other Contributors\n\n<a href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2\">Prof. Yaroslav Vyklyuk, DrSc, PhD</a>\n\n<a href=\"https://author.skills.network/instructors/nataliya_boyko\">Ass. Prof. Nataliya Boyko, PhD</a>\n\n## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                         |\n| ----------------- | ------- | ---------- | ---------------------------------------------------------- |\n|2023-03-18|01|Bohdan Kuno|Lab created|\n\n\n<hr>\n\n## <h3 align=\"center\"> © IBM Corporation 2023. All rights reserved. <h3/>\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}